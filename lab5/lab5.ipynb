{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYRMY7O1yZty5l6wedVUaQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JakubChmielewskiRepo/wstep-do-uczenia-maszynowego-175ic/blob/master/lab5/lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqSldrbWF6-M",
        "outputId": "313139c2-3d5e-4f40-8ebc-1629c4d15656"
      },
      "source": [
        "# !pip install spacy --upgrade\n",
        "# !spacy download pl_core_news_sm\n",
        "# !pip install textacy --upgrade\n",
        "\n",
        "import spacy\n",
        "import pl_core_news_sm \n",
        "import re\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from collections import Counter\n",
        "from spacy import displacy\n",
        "from spacy.matcher import Matcher\n",
        "import textacy\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "nlp = pl_core_news_sm.load()\n",
        "\n",
        "\n",
        "#Czytanie Stringa\n",
        "introduction_text = ('This tutorial is abałt Natural Language Processing in Spacy.')\n",
        "introduction_doc = nlp(introduction_text)\n",
        "# Extract tokens for the given doc\n",
        "print ([token.text for token in introduction_doc])\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'tutorial', 'is', 'abałt', 'Natural', 'Language', 'Processing', 'in', 'Spacy', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwK9th-NH8qr",
        "outputId": "d57bb92c-cfe6-41fb-9bc9-3c4d9653531f"
      },
      "source": [
        "#Czytanie pliku tekstowego\n",
        "file_name = 'introduction.txt'\n",
        "introduction_file_text = ('Wczoraj o godzinie Jakuba Chmielewskiego osiemnastej odbyło się zebranie Stowarzyszenia Bełchatowskich Rodzin, na którym omawiano drażniący problem lanego poniedziałku. Wysunięto także propozycję, aby na ulicę wyszły prewencyjnie liczniejsze niż zwykle patrole policji i straży miejskiej. Rodziny zaapelowały do młodzieży, żeby ta nie przesadzała z wodą. Tymczasem sama młodzież mówi, że nic złego się nigdy nie dzieje. „My się oblewamy między sobą, serio. Na pewno do żadnego dorosłego kolesia z butlami nie przylecimy. Nie wiem, czemu wszyscy tak się tego poniedziałku boją!”. Koledzy mojego rozmówcy tylko przytakują głowami.')\n",
        "introduction_file_doc = nlp(introduction_file_text)\n",
        "# Extract tokens for the given doc\n",
        "print ([token.text for token in introduction_file_doc])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Wczoraj', 'o', 'godzinie', 'Jakuba', 'Chmielewskiego', 'osiemnastej', 'odbyło', 'się', 'zebranie', 'Stowarzyszenia', 'Bełchatowskich', 'Rodzin', ',', 'na', 'którym', 'omawiano', 'drażniący', 'problem', 'lanego', 'poniedziałku', '.', 'Wysunięto', 'także', 'propozycję', ',', 'aby', 'na', 'ulicę', 'wyszły', 'prewencyjnie', 'liczniejsze', 'niż', 'zwykle', 'patrole', 'policji', 'i', 'straży', 'miejskiej', '.', 'Rodziny', 'zaapelowały', 'do', 'młodzieży', ',', 'żeby', 'ta', 'nie', 'przesadzała', 'z', 'wodą', '.', 'Tymczasem', 'sama', 'młodzież', 'mówi', ',', 'że', 'nic', 'złego', 'się', 'nigdy', 'nie', 'dzieje', '.', '„', 'My', 'się', 'oblewamy', 'między', 'sobą', ',', 'serio', '.', 'Na', 'pewno', 'do', 'żadnego', 'dorosłego', 'kolesia', 'z', 'butlami', 'nie', 'przylecimy', '.', 'Nie', 'wiem', ',', 'czemu', 'wszyscy', 'tak', 'się', 'tego', 'poniedziałku', 'boją', '!', '”', '.', 'Koledzy', 'mojego', 'rozmówcy', 'tylko', 'przytakują', 'głowami', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8MhKvKsVN2c_",
        "outputId": "afab68e3-7440-4067-bfba-ff9e56c3b7d6"
      },
      "source": [
        "#Wypisywanie zdań z tekstu\n",
        "about_text = ('Gus Proto is a Python developer currently working for a London-based Fintech company. He is interested in learning Natural Language Processing.')\n",
        "about_doc = nlp(about_text)\n",
        "sentences = list(about_doc.sents)\n",
        "for sentence in sentences:\n",
        "  print(sentence)\n",
        "  "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gus Proto is a Python developer currently working for a London-based Fintech company.\n",
            "He is interested in learning Natural Language Processing.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0f-Nb_ERqYg",
        "outputId": "e26b24e4-d13c-4cc8-b2c4-67613e07c8aa"
      },
      "source": [
        "#Porównanie wypisywania zdań z customowym znacznikiem, oraz ze znacznikiem domyślnym\n",
        "def set_custom_boundaries(doc):\n",
        "# Adds support to use `...` as the delimiter for sentence detection\n",
        "  for token in doc[:-1]:\n",
        "    if token.text == '...':\n",
        "      doc[token.i+1].is_sent_start = True\n",
        "  return doc\n",
        "\n",
        "ellipsis_text = ('Gus, can you, ... never mind, I forgot what I was saying. So, do you think we should ...')\n",
        "# Load a new model instance\n",
        "custom_nlp = spacy.load('en_core_web_sm')\n",
        "custom_nlp.add_pipe(set_custom_boundaries, before='parser')\n",
        "custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
        "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
        "for sentence in custom_ellipsis_sentences:\n",
        "  print(sentence)\n",
        "# Sentence Detection with no customization\n",
        "ellipsis_doc = nlp(ellipsis_text)\n",
        "ellipsis_sentences = list(ellipsis_doc.sents)\n",
        "for sentence in ellipsis_sentences:\n",
        "  print(sentence)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gus, can you, ...\n",
            "never mind, I forgot what I was saying.\n",
            "So, do you think we should ...\n",
            "Gus, can you, ... never mind, I forgot what I was saying.\n",
            "So, do you think we should ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cE1kGDy89zg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11b034b6-a9e8-4c93-9c31-90849f404c08"
      },
      "source": [
        "#Tokenizacja\n",
        "# for token in introduction_file_doc:\n",
        "#   print (token, token.idx)\n",
        "\n",
        "# for token in introduction_file_doc:\n",
        "#   print (token, token.idx, token.text_with_ws,token.is_alpha, token.is_punct, token.is_space,token.shape_, token.is_stop)\n",
        "\n",
        "\n",
        "prefix_re = spacy.util.compile_prefix_regex(custom_nlp.Defaults.prefixes)\n",
        "suffix_re = spacy.util.compile_suffix_regex(custom_nlp.Defaults.suffixes)\n",
        "infix_re = re.compile(r'''[-~]''')\n",
        "def customize_tokenizer(nlp):\n",
        "# Adds support to use `-` as the delimiter for tokenization\n",
        " return Tokenizer(nlp.vocab, prefix_search=prefix_re.search,suffix_search=suffix_re.search,infix_finditer=infix_re.finditer,token_match=None)\n",
        "\n",
        "custom_nlp.tokenizer = customize_tokenizer(custom_nlp)\n",
        "custom_tokenizer_about_doc = custom_nlp(introduction_text)\n",
        "print([token.text for token in custom_tokenizer_about_doc])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'tutorial', 'is', 'abałt', 'Natural', 'Language', 'Processing', 'in', 'Spacy', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO9zbPhuEFtt",
        "outputId": "8e723860-5bc8-451a-b13a-5b695b49c68d"
      },
      "source": [
        "#Słowa pomijalne\n",
        "import spacy\n",
        "spacy_stopwords = spacy.lang.pl.stop_words.STOP_WORDS\n",
        "len(spacy_stopwords)\n",
        "326\n",
        "for stop_word in list(spacy_stopwords)[:10]:\n",
        "  print(stop_word)\n",
        "\n",
        "#Usuwanie pomijalnych słów\n",
        "for token in introduction_file_doc:\n",
        "  if not token.is_stop:\n",
        "    print (token)\n",
        "\n",
        "about_no_stopword_doc = [token for token in introduction_file_doc if not token.is_stop]\n",
        "print (about_no_stopword_doc)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ty\n",
            "was\n",
            "wszystko\n",
            "podczas\n",
            "one\n",
            "przed\n",
            "tego\n",
            "mam\n",
            "mogą\n",
            "je\n",
            "Wczoraj\n",
            "godzinie\n",
            "osiemnastej\n",
            "odbyło\n",
            "zebranie\n",
            "Stowarzyszenia\n",
            "Bełchatowskich\n",
            "Rodzin\n",
            ",\n",
            "omawiano\n",
            "drażniący\n",
            "problem\n",
            "lanego\n",
            "poniedziałku\n",
            ".\n",
            "Wysunięto\n",
            "propozycję\n",
            ",\n",
            "ulicę\n",
            "wyszły\n",
            "prewencyjnie\n",
            "liczniejsze\n",
            "zwykle\n",
            "patrole\n",
            "policji\n",
            "straży\n",
            "miejskiej\n",
            ".\n",
            "Rodziny\n",
            "zaapelowały\n",
            "młodzieży\n",
            ",\n",
            "przesadzała\n",
            "wodą\n",
            ".\n",
            "Tymczasem\n",
            "młodzież\n",
            "mówi\n",
            ",\n",
            "złego\n",
            "dzieje\n",
            ".\n",
            "„\n",
            "oblewamy\n",
            ",\n",
            "serio\n",
            ".\n",
            "pewno\n",
            "żadnego\n",
            "dorosłego\n",
            "kolesia\n",
            "butlami\n",
            "przylecimy\n",
            ".\n",
            "wiem\n",
            ",\n",
            "poniedziałku\n",
            "boją\n",
            "!\n",
            "”\n",
            ".\n",
            "Koledzy\n",
            "mojego\n",
            "rozmówcy\n",
            "przytakują\n",
            "głowami\n",
            ".\n",
            "[Wczoraj, godzinie, osiemnastej, odbyło, zebranie, Stowarzyszenia, Bełchatowskich, Rodzin, ,, omawiano, drażniący, problem, lanego, poniedziałku, ., Wysunięto, propozycję, ,, ulicę, wyszły, prewencyjnie, liczniejsze, zwykle, patrole, policji, straży, miejskiej, ., Rodziny, zaapelowały, młodzieży, ,, przesadzała, wodą, ., Tymczasem, młodzież, mówi, ,, złego, dzieje, ., „, oblewamy, ,, serio, ., pewno, żadnego, dorosłego, kolesia, butlami, przylecimy, ., wiem, ,, poniedziałku, boją, !, ”, ., Koledzy, mojego, rozmówcy, przytakują, głowami, .]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJJ4P-twFg5N",
        "outputId": "7e17ca7a-6b8d-475f-e3b4-9cce5d5256c0"
      },
      "source": [
        "#Lemmatyzacja - grupowanie słów\n",
        "conference_help_text = ('Gus is helping organize a developer conference on Applications of Natural Language Processing. He keeps organizing local Python meetups and several internal talks at his workplace.')\n",
        "conference_help_doc = nlp(conference_help_text)\n",
        "for token in conference_help_doc:\n",
        "  print (token, token.lemma_)\n",
        "\n",
        "  "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gus gus\n",
            "is is\n",
            "helping helping\n",
            "organize organize\n",
            "a a\n",
            "developer developer\n",
            "conference conference\n",
            "on on\n",
            "Applications applications\n",
            "of of\n",
            "Natural natural\n",
            "Language language\n",
            "Processing processing\n",
            ". .\n",
            "He he\n",
            "keeps keeps\n",
            "organizing organizing\n",
            "local local\n",
            "Python python\n",
            "meetups meetups\n",
            "and and\n",
            "several several\n",
            "internal internal\n",
            "talks talks\n",
            "at at\n",
            "his his\n",
            "workplace workplace\n",
            ". .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_xaK_hZGAfC",
        "outputId": "e3ff32b0-2e8a-4ce8-d92d-8638fb16c377"
      },
      "source": [
        "#Słowa powtarzające się i unikalne\n",
        "from collections import Counter\n",
        "complete_text = ('W ubiegłym roku śmigus- dyngus okazał się wyjątkowo mokry Jakub Chmielewski i wyjątkowo brzemienny w skutkach. W wyniku „niewinnych” igraszek z wodą doszło do wypadku samochodowego, w wyniku którego jedna osoba doznała poważnych obrażeń kręgosłupa. Było to na szczęście jedyne tak tragiczne wydarzenie owego dnia.')\n",
        "\n",
        "\n",
        "complete_doc = nlp(complete_text)\n",
        "# Remove stop words and punctuation symbols\n",
        "words = [token.text for token in complete_doc\n",
        "  if not token.is_stop and not token.is_punct]\n",
        "word_freq = Counter(words)\n",
        "# 5 commonly occurring words with their frequencies\n",
        "common_words = word_freq.most_common(5)\n",
        "print (common_words)\n",
        "\n",
        "# Unique words\n",
        "unique_words = [word for (word, freq) in word_freq.items() if freq == 1]\n",
        "print (unique_words)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('wyjątkowo', 2), ('wyniku', 2), ('ubiegłym', 1), ('śmigus', 1), ('dyngus', 1)]\n",
            "['ubiegłym', 'śmigus', 'dyngus', 'okazał', 'mokry', 'Jakub', 'Chmielewski', 'brzemienny', 'skutkach', 'niewinnych', 'igraszek', 'wodą', 'doszło', 'wypadku', 'samochodowego', 'osoba', 'doznała', 'poważnych', 'obrażeń', 'kręgosłupa', 'szczęście', 'jedyne', 'tragiczne', 'wydarzenie', 'owego', 'dnia']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx9cn5CVGsFV",
        "outputId": "aff7a3a7-43d9-4269-864e-f5c442f8d3f0"
      },
      "source": [
        "#Rozpoznawanie częsci mowy\n",
        "for token in complete_doc:\n",
        "  print (token, token.tag_, token.pos_, spacy.explain(token.tag_))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W PREP ADP None\n",
            "ubiegłym ADJ ADJ adjective\n",
            "roku SUBST NOUN None\n",
            "śmigus SUBST NOUN None\n",
            "- INTERP PUNCT None\n",
            "dyngus SUBST NOUN None\n",
            "okazał PRAET VERB None\n",
            "się QUB PART None\n",
            "wyjątkowo ADV ADV adverb\n",
            "mokry ADJ ADJ adjective\n",
            "i CONJ CCONJ conjunction\n",
            "wyjątkowo ADV ADV adverb\n",
            "brzemienny ADJ ADJ adjective\n",
            "w PREP ADP None\n",
            "skutkach SUBST NOUN None\n",
            ". INTERP PUNCT None\n",
            "W PREP ADP None\n",
            "wyniku SUBST NOUN None\n",
            "„ INTERP PUNCT None\n",
            "niewinnych ADJ ADJ adjective\n",
            "” INTERP PUNCT None\n",
            "igraszek SUBST NOUN None\n",
            "z PREP ADP None\n",
            "wodą SUBST NOUN None\n",
            "doszło PRAET VERB None\n",
            "do PREP ADP None\n",
            "wypadku SUBST NOUN None\n",
            "samochodowego ADJ ADJ adjective\n",
            ", INTERP PUNCT None\n",
            "w PREP ADP None\n",
            "wyniku SUBST NOUN None\n",
            "którego ADJ ADJ adjective\n",
            "jedna ADJ ADJ adjective\n",
            "osoba SUBST NOUN None\n",
            "doznała PRAET VERB None\n",
            "poważnych ADJ ADJ adjective\n",
            "obrażeń SUBST NOUN None\n",
            "kręgosłupa SUBST NOUN None\n",
            ". INTERP PUNCT None\n",
            "Było PRAET VERB None\n",
            "to SUBST NOUN None\n",
            "na PREP ADP None\n",
            "szczęście SUBST NOUN None\n",
            "jedyne ADJ ADJ adjective\n",
            "tak ADV ADV adverb\n",
            "tragiczne ADJ ADJ adjective\n",
            "wydarzenie SUBST NOUN None\n",
            "owego ADJ ADJ adjective\n",
            "dnia SUBST NOUN None\n",
            ". INTERP PUNCT None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGJN0kXnJT-9",
        "outputId": "b1552779-4d7a-48d3-8c65-c27e65e99246"
      },
      "source": [
        "#Filtrowanie słów według kategorii:\n",
        "nouns = []\n",
        "adjectives = []\n",
        "for token in introduction_file_doc:\n",
        "    if token.pos_ == 'NOUN':\n",
        "        nouns.append(token)\n",
        "    if token.pos_ == 'ADJ':\n",
        "        adjectives.append(token)\n",
        "\n",
        "print(f'Rzeczowniki: {nouns}')\n",
        "print(f'Przymiotniki: {adjectives}')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rzeczowniki: [godzinie, zebranie, Stowarzyszenia, Rodzin, problem, poniedziałku, propozycję, ulicę, prewencyjnie, patrole, policji, straży, Rodziny, młodzieży, wodą, młodzież, nic, kolesia, butlami, poniedziałku, Koledzy, rozmówcy, głowami]\n",
            "Przymiotniki: [osiemnastej, Bełchatowskich, którym, lanego, liczniejsze, miejskiej, ta, sama, złego, żadnego, dorosłego, wszyscy, tego, mojego]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiMfHCp8KLgk",
        "outputId": "5e04293e-cbc6-4695-f8f1-ab9a947078a9"
      },
      "source": [
        "#Wizualizacja displaCy\n",
        "about_displaCy_text = \"To jest testowy tekst po polsku, nic on nie znaczy i jest bez sensu\"\n",
        "about_displaCy_doc = nlp(about_displaCy_text)\n",
        "displacy.serve(about_displaCy_doc, style='dep')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkUvOrfWLNAV",
        "outputId": "c42d999e-ac62-42b3-a7bb-e265cf92310e"
      },
      "source": [
        "#Funkcje przetwarzania wstępnego\n",
        "def is_token_allowed(token):\n",
        "    if not token or not token.string.strip() or token.is_stop or token.is_punct:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def preprocess_token(token):\n",
        "    return token.lemma_.strip().lower()\n",
        "\n",
        "complete_filtered_tokens = [preprocess_token(token)for token in complete_doc if is_token_allowed(token)]\n",
        "print(complete_filtered_tokens)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ubiegły', 'śmigus', 'dyngus', 'okazać', 'wyjątkowo', 'mokry', 'wyjątkowo', 'brzemienny', 'skutek', 'wynik', 'winny', 'igraszka', 'woda', 'dojść', 'wypadek', 'samochodowy', 'wynik', 'osoba', 'doznać', 'poważny', 'obrażenie', 'kręgosłup', 'szczęście', 'jedyny', 'tragiczny', 'wydarzenie', 'ów', 'dzień']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OdTw-2tuMP5u",
        "outputId": "4ce6d9d5-889d-4e0b-8d72-e05326f11923"
      },
      "source": [
        "#Ekstrakcja numeru telefonu \n",
        "\n",
        "phone_number=(\"cos tam 222 333 444 cos tam\")\n",
        "\n",
        "def extract_phone_number(nlp_doc):\n",
        "\tpattern = [{\"SHAPE\": \"ddd\"}, {\"SHAPE\": \"ddd\"},{\"SHAPE\": \"ddd\"}]\n",
        "\tmatcher.add('PHONE_NUMBER', None, pattern)\n",
        "\tmatches = matcher(nlp_doc)\n",
        "\tfor match_id, start, end in matches:\n",
        "\t\tspan = nlp_doc[start:end]\n",
        "\t\treturn span.text\n",
        "\n",
        "conference_org_doc = nlp(phone_number)\n",
        "extract_phone_number(conference_org_doc)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'222 333 444'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ek5LRaEENy7A",
        "outputId": "987d291b-3891-41c4-f432-bb6a0e38e9a5"
      },
      "source": [
        "#Struktura zdania\n",
        "\n",
        "text = ('Daj mi tę noc, te jedną noc')\n",
        "text_doc = nlp(text)\n",
        "for token in text_doc:\n",
        "  print (token.text, token.tag_, token.head.text, token.dep_)\n",
        "displacy.serve(text_doc, style='dep')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Daj IMPT Daj ROOT\n",
            "mi PPRON12 Daj iobj\n",
            "tę ADJ noc det\n",
            "noc SUBST Daj obj\n",
            ", INTERP noc punct\n",
            "te ADJ noc det\n",
            "jedną ADJ noc amod\n",
            "noc SUBST Daj obl\n",
            "\n",
            "Using the 'dep' visualizer\n",
            "Serving on http://0.0.0.0:5000 ...\n",
            "\n",
            "Shutting down server on port 5000.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQa9J29JORSa",
        "outputId": "46ba0a29-132d-4b75-be06-2484bbbbfb8b"
      },
      "source": [
        "#Drzewa\n",
        "\n",
        "one_line_about_text = ('Chciałbym być marynarzem chciałbym mieć tatuaże')\n",
        "one_line_about_doc = nlp(one_line_about_text)\n",
        "\n",
        "# Wyświetlenie sąsiadów słowa \"chciałbym\"\n",
        "print([token.text for token in one_line_about_doc[3].children])\n",
        "print (one_line_about_doc[3].nbor(-1))\n",
        "print (one_line_about_doc[3].nbor())\n",
        "#Poddrzewo słowa \"chciałbym\"\n",
        "print (list(one_line_about_doc[3].subtree))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['być', 'marynarzem', 'mieć']\n",
            "marynarzem\n",
            "mieć\n",
            "[być, marynarzem, chciałbym, mieć, tatuaże]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ6vhTgoPjMp"
      },
      "source": [
        "#Wykrywanie fraz rzeczownikowych\n",
        "conference_text = ('Jedzenie korzenie samouwielbienie')\n",
        "conference_doc = nlp(conference_text)\n",
        "for chunk in conference_doc.noun_chunks:\n",
        "  print (chunk)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLfUvecjQEPx",
        "outputId": "5ffbcd49-2b81-43e8-a876-480f5510605e"
      },
      "source": [
        "about_talk_text=('Komiczny komik terroryzował miasto mając jedynie lwa i płonącą obręcz')\n",
        "pattern = [{\"POS\": \"VERB\", \"OP\": \"*\"},{\"POS\": \"VERB\", \"OP\": \"+\"}]\n",
        "about_talk_doc = textacy.make_spacy_doc(about_talk_text, nlp)\n",
        "verb_phrases = textacy.extract.matches(about_talk_doc, pattern)\n",
        "for chunk in verb_phrases:\n",
        "    print(chunk.text)\n",
        "\n",
        "for chunk in about_talk_doc.noun_chunks:\n",
        "    print (chunk)\n"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "terroryzował\n",
            "mając\n",
            "płonącą\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RaH0_CvKRrMP",
        "outputId": "8a035245-ef85-4858-d914-7ba7facff3ed"
      },
      "source": [
        "#Ukrywanie imienia i nazwiska\n",
        "survey_text =('Tatiana Trąbalska wcale nie ma trąby jak twierdzi nasz specjalny wysłannik Tomasz Beztrąby')\n",
        "def replace_person_names(token):\n",
        "    if token.ent_iob != 0 and token.ent_type_ == 'persName':\n",
        "        return 'XYZ '\n",
        "    return token.string\n",
        "\n",
        "def redact_names(nlp_doc):\n",
        "    for ent in nlp_doc.ents:\n",
        "        ent.merge()\n",
        "    tokens = map(replace_person_names, nlp_doc)\n",
        "    return ''.join(tokens)\n",
        "\n",
        "survey_doc = nlp(survey_text)\n",
        "print(redact_names(survey_doc))\n",
        "print(survey_text)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "XYZ wcale nie ma trąby jak twierdzi nasz specjalny wysłannik XYZ \n",
            "Tatiana Trąbalska wcale nie ma trąby jak twierdzi nasz specjalny wysłannik Tomasz Beztrąby\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2VNOEXXRG1m"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}